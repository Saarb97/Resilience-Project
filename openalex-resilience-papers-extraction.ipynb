{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10721723,"sourceType":"datasetVersion","datasetId":6646282}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyalex networkx ijson","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T13:38:26.304004Z","iopub.execute_input":"2025-03-02T13:38:26.304426Z","iopub.status.idle":"2025-03-02T13:38:33.668473Z","shell.execute_reply.started":"2025-03-02T13:38:26.304392Z","shell.execute_reply":"2025-03-02T13:38:33.667024Z"}},"outputs":[{"name":"stdout","text":"Collecting pyalex\n  Downloading pyalex-0.16-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\nCollecting ijson\n  Downloading ijson-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pyalex) (2.32.3)\nRequirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pyalex) (2.2.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pyalex) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pyalex) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pyalex) (2024.8.30)\nDownloading pyalex-0.16-py3-none-any.whl (11 kB)\nDownloading ijson-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: ijson, pyalex\nSuccessfully installed ijson-3.3.0 pyalex-0.16\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pyalex\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport json\nimport os\nfrom pyalex import Works\nfrom tqdm import tqdm\n\n# Set OpenAlex API email\npyalex.config.email = \"email here\"\n\n# Paths to existing dataset files\nSEED_WORKS_PATH = \"/kaggle/input/social-resilience-data/seed_works.json\"\nREFERENCE_METADATA_PATH = \"/kaggle/input/social-resilience-data/reference_metadata.json\"\nlocal_seed_path = 'seed_works.json'\nlocal_metadata_path = 'reference_metadata.json'\n\ndef load_local_data():\n    \"\"\"Load local data from JSON files if available.\"\"\"\n    seed_works = []\n    reference_metadata = {}\n    \n    if os.path.exists(SEED_WORKS_PATH):\n        with open(SEED_WORKS_PATH, \"r\") as f:\n            seed_works = json.load(f)\n    \n    if os.path.exists(REFERENCE_METADATA_PATH):\n        with open(REFERENCE_METADATA_PATH, \"r\") as f:\n            reference_metadata = json.load(f)\n    \n    return seed_works, reference_metadata\n\ndef save_local_data(seed_works, reference_metadata):\n    \"\"\"Save updated data to JSON files.\"\"\"\n    with open(local_seed_path, \"w\") as f:\n        json.dump(list(seed_works), f, indent=4)\n    \n    with open(local_metadata_path, \"w\") as f:\n        json.dump(reference_metadata, f, indent=4)\n\ndef fetch_seed_works(query=\"social resilience\", per_page=100, max_pages=100):\n    \"\"\"Fetch new seed works while avoiding duplicates.\"\"\"\n    existing_seed_ids = {work[\"id\"] for work in seed_works}\n    new_seed_works = []\n    \n    pager = (\n        Works()\n        .search_filter(title=query)\n        .paginate(method=\"page\", per_page=per_page)\n    )\n    \n    for page_index, page in enumerate(pager):\n        if page_index == max_pages:\n            break\n        if not page:\n            break\n        \n        for work in page:\n            if work[\"id\"] not in existing_seed_ids:\n                new_seed_works.append(work)\n    \n    return new_seed_works\n\ndef bulk_fetch_metadata_for_ids(ids_list, batch_size=50):\n    \"\"\"Fetch metadata for missing referenced works.\"\"\"\n    metadata_dict = {}\n    ids_list = list(ids_list)\n    \n    for start_idx in tqdm(range(0, len(ids_list), batch_size), desc=\"Fetching metadata batches\"):\n        batch = ids_list[start_idx : start_idx + batch_size]\n        joined_ids = \"|\".join(batch)\n        \n        pager = (\n            Works()\n            .filter(openalex=joined_ids)\n            .paginate(method=\"page\", per_page=batch_size)\n        )\n        \n        temp_data = []\n        for page_result in pager:\n            if not page_result:\n                break\n            temp_data.extend(page_result)\n        \n        for record in temp_data:\n            record_id = record.get(\"id\")\n            if record_id:\n                metadata_dict[record_id] = record\n    \n    return metadata_dict\n\n# Load existing data\nseed_works, reference_metadata = load_local_data()\nprint(f\"Loaded {len(seed_works)} existing seed works and {len(reference_metadata)} referenced works.\")\n\n# Fetch new seed works\nnew_seed_works = fetch_seed_works(query=\"resilience\", per_page=100, max_pages=100)\nprint(f\"Fetched {len(new_seed_works)} new seed works.\")\n\n# Merge new and existing seed works\nseed_works_dict = {work[\"id\"]: work for work in seed_works}\nfor work in new_seed_works:\n    seed_works_dict[work[\"id\"]] = work\nseed_works = list(seed_works_dict.values())\n\n# Collect all referenced work IDs\nall_ref_ids = set(ref for work in seed_works for ref in work.get(\"referenced_works\", []))\nexisting_ref_ids = set(reference_metadata.keys())\nmissing_ref_ids = all_ref_ids - existing_ref_ids\n\nprint(f\"Found {len(all_ref_ids)} unique referenced works. {len(missing_ref_ids)} are missing.\")\n\n# Fetch missing references\nif missing_ref_ids:\n    new_reference_metadata = bulk_fetch_metadata_for_ids(ids_list=missing_ref_ids, batch_size=50)\n    reference_metadata.update(new_reference_metadata)\n    print(f\"Fetched metadata for {len(new_reference_metadata)} new referenced works.\")\n\n# Ensure reference_metadata is correctly retained\nreference_metadata = {**reference_metadata, **new_reference_metadata}\n\n# Save updated data\nsave_local_data(seed_works, reference_metadata)\nprint(\"Updated dataset saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T13:38:33.814399Z","iopub.execute_input":"2025-03-02T13:38:33.814831Z","iopub.status.idle":"2025-03-02T13:38:33.833145Z","shell.execute_reply.started":"2025-03-02T13:38:33.814789Z","shell.execute_reply":"2025-03-02T13:38:33.832163Z"}},"outputs":[],"execution_count":9}]}